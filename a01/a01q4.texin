Consider the following pseudocode.
\begin{algorithm}
power(base, exp)
	if exp = 1 then // base case
		return base

	let base2 = base * base
	let newexp = exp / 2 // can achieve O(1) with bit-shifting; value is truncated

	if exp is odd then // can achieve O(1) with bit-masking
		return base * power(base2, newexp)
	else
		return power(base2, newexp)
\end{algorithm}
To view the algorithm, think of $n$ as an array of stones ($base$).
\begin{itemize}
\item
Two stones are being paired together and being put in a bag, so that there are $\frac{n}{2}$ bags.
This is the division process.
\item
If there is an remaining stone, i.e. $n$ is odd, then we put it into the last bag and deal with it when recursion call returns, in the same stack.
This is okay because real numbers are commutative.
\item 
Continue pairing until there is only one bag left with a value of $N_k$ (some square power of the original $base$, i.e. $base_{original}^{2^k}$), where $k$ is the number of times of the division process happened.
\item
Start fixing the odd cases.
That is, 
\begin{align*}
power(base, exp) &= N_k \times ifOdd(k-1,N_{k-1}) \times \cdots \times ifOdd(1, N_{1})
\end{align*}
where $ifOdd(m,N_{m})$ checks whether the $m$-th resursion step is an odd case; if so return $N_m$ (the value of $base$ at the $m$-th recursion call).
Again, each multiplication is done within its recursion stack -- no extra combining process.
This is the conquering process.
\item
Done
\end{itemize}

Notice here the concern is how many pairs (analogy above) the algorithm has processed, but not the number of multiplications.
Thus, the running time is the number of pairs can be formed, which is equivalent to the reverse of splitting an array by half -- like bottom-up vs. top-down.
\begin{align*}
T(n) &= k = T(\frac{n}{2})\\
	&= \Theta(log(n))
\end{align*}
\done
